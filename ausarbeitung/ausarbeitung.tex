\documentclass[a4,german]{article}

\usepackage[german]{babel} % für deutsche Silbentrennung und generierte Texte
\usepackage[T1]{fontenc}    % für deutsche Umlaute etc. in der Ausgabe
\usepackage[utf8]{inputenc} % für deutsche Umlaute.
\usepackage{graphicx}       % um Bilder einzubinden
\usepackage{subfigure} 	    % um 2 Bilder nebeneinander haben zu können
\usepackage{hyperref}       % um URLs korrekt einzubinden und Hyperlinks im Dokument zu ermöglichen
\usepackage{pdfpages}
\usepackage{cite}
\usepackage{babelbib}


\begin{document}

\title{Cloud Computing mal anders: Klassifikation von Wolkenbildern mit kNN und CNNs}
\author{Maximilian Birkenhagen, Ali Ebrahimi, Thilo Fryen,\\Lukas Hintze}
\maketitle

\begin{abstract}
    Wir vergleichen zwei Ansätze zur Klassifizierung von Wolken anhand von Bildern:
        einerseits einen \glqq klassischen\grqq\ Ansatz, in dem auf Grundlage von manuell ausgewählten Merkmalen eine \emph{$k$-nearest-neighbor}-Klassifika\-tion durchgeführt wird;
        andererseits einen auf \emph{Machine Learning} basierendem Ansatz, in dem die Klassifizierung durch ein neuronales Netz auf Grundlage von einem vortrainierten \emph{Convolutional Neural Network} extrahierten Merkmalen klassifiziert werden.
    Wir kommen zum Schluss, dass letzterer Ansatz deutlich besser geeignet ist, aber in beiden Varianten noch Potential für weitere Verbesserungen vorhanden ist.
\end{abstract}

\section{Einleitung}

Im 19. Jahrhundert begannen Wissenschaftler damit, Wolken zu typisieren, und schufen damit die Grundlage der heutigen Forschung.
\glqq Wolken sind ein wesentlicher Bestandteil der Atmosphäre. [...] Sie sind außerordentlich typische Kennzeichen der jeweiligen Wetterlage und Vorzeichen für die künftige Wetterentwicklung.\grqq\cite{wolkenatlas:Karlsruhe}
Daher ist die Klassifikation von Wolken nicht nur für Wolken-Enthusiasten interessant, sondern spielt auch eine große Rolle in der Klimaforschung, bei Wettervorhersagen und bei Unwetterwarnungen.
All dies sind Gründe, warum wir uns mit der Wolkenklassifizierung anhand von Bildern beschäftigen---neben dem reinen Interesse an der Herausforderung, etwas maschinell zu klassifizieren, dass wir, die Autoren, selbst nicht sehr gut klassifizieren können.

Allerdings werden bei der heutigen Bestimmung andere Messmethoden wie Radar, Laser (z.B. LiDAR) oder Radiometer präferiert, bei denen Messungen von Sensoren an verschiedenen Orten (Satellit, Bodenstationen) miteinander kombiniert werden \cite{wang}.
Es gab aber auch schon zuvor Versuche, Wolken nur mit Hilfe von Bildern zu klassifizieren:
Forscher der Universität Kiel fotografierten den gesamten Himmel immer wieder von derselben Position aus und unterschieden dabei sieben verschiedene Wolkenarten(kombinationen), darunter auch den \glqq leeren Himmel\grqq.
Mithilfe eines k-nearest-neighbour Klassifikators erreichten sie eine Genauigkeit von 97\% \cite{heinle}.


\subsection{Unsere Arbeit}

In dieser Arbeit versuchen wir, Wolken automatisiert zu klassifizieren, und vergleichen dabei zwei unterschiedliche Ansätze:
Ein Ansatz basiert auf \glqq klassischen\grqq\ Methoden der Computer Vision, das heißt auf händisch ausgewählten Merkmalen wie Mittelwert, Standardabweichung, Farb-Histogrammen und Kantenzählung, auf deren Grundlage dann ein k-nearest-neighbor-Klassifikator angewendet wird.
Ein Anderer basiert auf den \glqq moderneren\grqq\ Convolutional Neural Networks (CNN), die Merkmale extrahieren, auf deren Grundlagen dann die weitere Klassifikation durch ein konventionelles neuronales Netz stattfindet.


\subsection{Aufbau der Arbeit}

In Abschnitt \ref{sec:daten} erläutern wir die Daten, die unserer Arbeit zugrunde liegen, also deren Herkunft, deren Charakteristika und unsere Klasseneinteilung.
In Abschnitt \ref{sec:methodik} folgt die Darlegung unserer Klassifizierungsmethodik, in der auf die Details sowohl des klassischen als auch des auf Machine Learning basierenden Ansatzes eingegangen wird.
Abschließend werden beide Ansätze in Abschnitt \ref{sec:experimente} evaluiert und auf quantitativer Ebene verglichen. Hier erfolgt auch ein Vergleich mit der Leistung einer Gruppe von nicht fachkundigen Menschen.
Unser Fazit ist in Abschnitt \ref{sec:fazit} dargestellt.


\section{Daten}
\label{sec:daten}

Unser Datensatz besteht aus 833 Farbbildern, die zu ungefähr gleichen Teilen vom Wolkenatlas der Webseite wolken-online.de \cite{wolkenonline} und der Bildergalerie des \emph{International Cloud Atlas} der \emph{World Meteorological Organization} \cite{wmo:images} stammen.
Die Bilder sind meist von individuellen Personen geschossene Fotos vom Himmel.
Allerdings ist auf den Bildern oft nicht nur der Himmel zu sehen, sondern auch vor allem am unteren Rand des Bildes die Erdoberfläche mitsamt Objekten wie Häusern oder Bäumen.
Außerdem sind gelegentlich mehrere Wolkenarten auf demselben Bild vertreten.

Der \textit{International Cloud Atlas}\cite{internationalCloudAtlas} unterscheidet zehn Wolkengattungen, welche in Abbildung~\ref{fig:cloudtypes} schematisch dargestellt sind.
Die Wolkenarten unterscheiden sich im Wesentlichen durch ihre Struktur, Dichte und Höhe, wobei die letzten beiden Eigenschaften auf Bildern nur schwer bis gar nicht erkennbar sind.

\begin{figure}[h!]
\centering
\includegraphics[height=11cm,keepaspectratio]{Cloud_infographic-01.png}
\caption{Schematische Darstellung der zehn Wolkengattungen vom britischen Met Office\cite{metoffice}}
    \label{fig:cloudtypes}
\end{figure}

Aufgrund der Ähnlichkeit einiger Klassen und der doch recht geringen Anzahl an Bildern im Vergleich zu der Anzahl an Klassen entschieden wir uns für eine Gruppierung in die vier Oberklassen \glqq Cirriform\grqq, \glqq Stratiform\grqq, \glqq Cumuliform\grqq, sowie \glqq Stratocumuliform\grqq, die in Tabelle \ref{table:oberklassen} beschrieben und in Abbildung \ref{fig:klassen} zu sehen sind.

\begin{figure}[h!]
	\subfigure[Cirriform]
	{\includegraphics[width=0.53\textwidth]{cirriform7}}
	\subfigure[Stratiform]
	{\includegraphics[width=0.53\textwidth]{stratiform30}}
	\\
	\subfigure[Cumuliform]
	{\includegraphics[width=0.53\textwidth]{cumuliform0}}
	\subfigure[Stratocumuliform]
	{\includegraphics[width=0.53\textwidth]{stratocumuliform171}}
	\caption{Übersicht der vier Oberklassen}
	\label{fig:klassen}
\end{figure}


\begin{table}[h!]
\begin{tabular}{l | p{3.5cm} | p{3.5cm} | l}
    Hauptklasse & Unterklassen & Beschreibung & \# Bilder \\ \hline
    Cirriform & Cirrus & gefedert, oft durchsichtig; sehr dünn und gefächert & 123 \\ \hline
    Cumuliform & Cumulus, Cumulonimbus & große Haufen, \glqq Schäfchenwolken\grqq & 294 \\ \hline
    Stratiform & Cirrostratus, Altostratus, Nimbostratus, Stratus & sehr flächig, zusammenhängend; meist durchgängige Bedeckung & 129 \\ \hline
    Stratocumuliform & Cirrocumulus, Altocumulus, Stratocumulus & viele kleine Haufen über eine große Fläche & 287 \\
\end{tabular}
    \caption{Zusammensetzung und Beschreibung unseres Klassifizierungsschemas}
    \label{table:oberklassen}
\end{table}

Wie man auch schon an den Namen der verschiedenen Wolkengattungen sieht, gehen die Klassen leider teils ineinander über, was die Unterscheidung deutlich erschwert.
Außerdem gibt es noch sehr viele Arten, Unterarten und Begleitwolken, die sich teilweise ebenso in ihren Eigenschaften überschneiden.

\section{Methodik}
\label{sec:methodik}

Im diesem Abschnitt erläutern wir die Methodik der beiden Klassifikationsansätze.
Eine Übersicht über den groben Ablauf kann in Abbildung \ref{fig:ablauf} gewonnen werden. 
Bevor mit der eigentlichen Klassifikation begonnen wird, werden die Ursprungsbilder zunächst möglichst so zugeschnitten, dass nur noch Himmel im Bild vorhanden ist (Abschnitt \ref{sec:vorverarbeitung}).
Ab hier unterscheidet sich der \glqq klassische\grqq von dem auf Machine Learning basierenden Ansatz.
Die beiden Varianten sind in Abschnitt \ref{sec:classic} bzw.\ Abschnitt \ref{sec:ml} beschrieben.

\begin{figure}
	\centering
	\includegraphics[height=0.25\paperheight]{Ablauf.pdf}
	\caption{Ablauf der Klassifikation}
	\label{fig:ablauf}
\end{figure}


\subsection{Vorverarbeitung der Bilder}
\label{sec:vorverarbeitung}
Da unsere ursprünglichen Bilder sich in ihren Dimensionen und in ihrer Art und Weise wie sie entstanden sind stark unterscheiden, mussten wir sie vor der eigentlichen Klassifikation anpassen.
Zuerst haben wir unpassende Bilder, wie zum Beispiel von einem Sturm oder Fotos, auf denen die Wolken kaum erkennbar waren, manuell aussortiert.
Wie erwähnt war oft der untere Rand des Bildes nicht mehr nur reiner Himmel. Oft waren auch Wiesen oder Bäume zu erkennen, die der Klassifizierung Probleme bereiteten.
Durch eine Binarisierung und unter Anwendung unseres BoxCut Algorithmus, die in Kapitel \ref{sec:binary} und Kapitel \ref{sec:boxcut} näher erläutert werden, konnten wir relativ akkurat den Himmel vom restlichen Bild trennen.
Zum Schluss haben wir die Bilder noch auf die einheitliche Größe von 500x500 Pixel gebracht, bei welcher die Algorithmen noch schnell ein genaues Ergebnis berechnen konnten.

\subsubsection{Binarisierung} 
\label{sec:binary}
Die Binarisierung haben wir mithilfe des arithmetischen Mittels über das gesamte Bild sowie dem Farbwert und der Helligkeit aus dem HSV-Farbraum implementiert.
So gehört  ein Pixel dann zum Vordergrund, falls er heller ist als der Durchschnitt der Helligkeit des gesamten Bildes und falls er weder grün noch sehr stark dunkel ist.
Dies hat nach vielem herum experimentieren für ein gutes Ergebnis gesorgt. \ref{fig:boxAlg}.

\subsubsection{BoxCut Algorithmus}
\label{sec:boxcut}
Nach dem Binarisieren versucht der BoxCut Algorithmus den eigentlichen Himmel vom Rest des Bildes auszuscheiden. Dazu wird von unten eine rechteckige Fläche des Bildes ausgewählt. Die Höhe beträgt dabei ca. 10\%
des Bildes und die gesamte Breite (dargestellt durch das rote Rechteck im mittleren binarisierten Bild in Abbildung \ref{fig:boxAlg}). In dem markierten Rechteck wird dann der durchschnittliche Farbwert berechnet.
Liegt dieser über unserem Schwellwert von 0.35, so gehört ein Teil der Box wahrscheinlich zum Himmel, sodass das Bild bis zum unteren Teil der Box geschnitten wird.
Falls aber der durchschnittliche Farbwert unter dem Schwellwert liegen sollte, so wird die Box um die Hälfte ihrer Höhe nach oben verschoben. Dies wird solange fortgeführt, bis letztlich der Schwellwert überschritten wird und das Bild zugeschnitten wird.


\begin{figure}[h!]
\centering
\includegraphics[width=1.1\textwidth]{boxAlg}
\caption{BoxCut-Algorithmus hat den Himmel erfolgreich von der Erde getrennt}
\label{fig:boxAlg}
\end{figure}


\subsection{Klassischer Ansatz}
\label{sec:classic}

Der \glqq klassische\grqq\ Ansatz beginnt mit der Bestimmung der Merkmale.
Neben den oft verwendeten Merkmalen Standardabweichung, Mittelwert und 1D-/3D-/Grau-Histogramm wird auch eine Kantenzählung durchgeführt, bei der die gefundenen Kanten aufsummiert werden (Abschnitt \ref{sec:kanten}).
Zum Schluss dieses Ansatzes nutzen wir nun einen \emph{nearest-neighbor}-Klassifikator, der die Merkmale des zu klassifizierenden Bildes mit denen der Bilder im Trainingsdatensatz vergleicht.
Der zugehörige Train/Validation-Split bei diesem Ansatz war 8:2.

\subsubsection{Kantenzählung}
\label{sec:kanten}
Ein Merkmal, welches wir neben dem Mittelwert, der Standardabweichung und den Farb-Histogrammen verwenden, misst die Anzahl der Kanten auf den Bildern.
Dazu wird zuerst das Bild in ein Graustufenbild umgewandelt, da man so nicht die Kanten für jeden Farbwert im RGB-Farbraum einzeln berechnen muss. Als nächstes wird ein Gaussfilter mit Sigma = 2 auf das Bild angewendet, damit nur die gröberen Kanten ins Gewicht fallen. Danach wird mit Hilfe des Sobel-Filters ein Kantenbild erzeugt.
Dort wo sich Kanten befinden, sind die Pixel heller, d.h. die Werte sind höher.
Diese Werte werden dann zeilenweise aufsummiert, sodass ein Histogramm wie auf dem rechten Bild in Abbildung~\ref{fig:kaz} entsteht. So können die Kanten pro Zeile von zwei Bildern verglichen werden.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Kantenzaehlung.png}
\caption{Visualisierung der Kantenzählungs-Funktion}
    \label{fig:kaz}
\end{figure}


\subsection{Machine-Learning-Ansatz}
\label{sec:ml}

Bei diesem basierten Ansatz wird im Training der doch recht kleine Datensatz durch Data Augmentation kompensiert (Abschnitt \ref{sec:augmentation}).
Es folgt die Merkmalsextraktion durch ein auf ImageNet vortrainiertes Convolutional Neural Network (CNN); die extrahierten Merkmale werden dann durch ein Klassifikationsnetz klassifiziert (Abschnitt \ref{sec:neural}).

20\% des Datensatzes wurden fürs Testen reserviert, der Rest der Bilder wurde fürs Training verwendet, wobei hier wiederum 20\% für die Validierung und 80\% fürs eigentliche Training verwendet wurden.


\subsubsection{Augmentation der Bilder}
\label{sec:augmentation}

Um trotz des recht kleinen Datensatzes akzeptable Ergebnisse zu erzielen, haben wir Data Augmentation eingesetzt.
Zuerst wurde jedes Original-Trainingsbild halbiert, sodass sich effektiv die Anzahl der Trainingsbilder verdoppelt.
Weiterhin wurde die in Keras verfügbare Data-Augmentation-Funktionalität genutzt, woduch im Training bilder zufällig gespiegelt, rotiert, geschert, sowie gezoomt wurden.
Beide Maßnahmen erhöhten die Genauigkeit des Endsystems deutlich.

Trainiert wurde auf 4096 Bildern---eine höhere Anzahl an Bildern brachte keine signifikante Genauigkeitsverbesserung.


\subsubsection{Merkmalsextraktion und Klassifizierung}
\label{sec:neural}

Bei der Merkmalsextraktion wurde darauf verzichtet, ein eigenes CNN zu konstruieren und zu trainieren.
Stattdessen verwenden wir, wieder aufgrund der geringen Datensatzgröße, ein in Keras vorhandenes, auf ImageNet vortrainiertes Netz.
Die Wahl fiel dabei auf \emph{Xception} \cite{xception}.
Wir wählten es aufgrund der (verglichen mit den anderen in Keras vorhanden Netzen) hohen Genauigkeit bzgl.\ ImageNet.\footnote{Zwar hat \emph{Xception} zum aktuellen Zeitpunkt nicht die höchste Genauigkeit---dieses Netz wäre \emph{InceptionResNetV2}, wobei dieses nur ca. einen Prozentpunkt besser abschneidet---ist aber 60\% kleiner als \emph{InceptionResNetV2} und auch deutlich performanter.}

Die von \emph{Xception} errechneten 2048 Merkmale wurden nun durch ein weiteres, von uns konstruiertes neuronales Netz konstruiert.
Hier folgen auf zwei Fully-connected Layer mit 128 Neuronen und ReLU-Aktivierung ein letzter Fully-connected Layer mit 4 Neuronen und Softmax-Aktivierung, die als Ausgabe dient.
Zwischen den Schichten wurden je 60\% der Verbindungen gekappt, um Overfitting vorzubeugen.

Das Klassifizierungsnetz wurde nun auf etwa 50 Epochen aus je 4096 (s.o.) trainiert.
Die Anzahl an Epochen wurde wurde durch Keras' \emph{Early Stopping}-Funktionalität mit einem Gedultwert von 30 Epochen bzgl.\ des Loss-Wertes auf den Validierungsdaten gewählt.
Das Training des Netzes erfolgte durch stochastischen Gradient Descent (SGD) mit dem Learning-Rate-Parameter bei 0,001 und Momentum bei 0,9, und \emph{categorical cross entropy} als Loss-Funktion.

In der Anwendung des Netzes auf Testfälle wurde nun der Input in drei verschiedene, sich teils überlappende, Segmente aufgeteilt (s. Abbildung \ref{fig:augmentation}), und die Klassifizierungspipeline dann auf jedes Segment angewendet, die dabei resultierenden Gewichte der Outputschicht summiert, und die Kategorie mit dem höchsten Gewicht als Klassifizierung gewählt.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{Augmentation}
\caption{Aus einem Originalbild werden drei verschiedene, aber sich überlappende Bilder}
    \label{fig:augmentation}
\end{figure}


\section{Experimente und Ergebnisse}
\label{sec:experimente}
In diesem Abschnitt werden die erzielten Ergebnisse unter verschiedenen Parametern vorgestellt.

\subsection{Klassischer Ansatz}%TODO: Daten bei Änderungen aktualisieren
Die höchste Genauigkeit, die wir mit dem klassischen Ansatz erzielen konnten, beträgt 49\%. Wie gut dabei die einzelnen Merkmale abschneiden, kann man aus Tabelle~\ref{tab:gen} herauslesen. Dabei steht \textit{mean} für den Mittelwert und \textit{std} für die Standardabweichung. 


\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|l|}
 \hline
 \textbf{Merkmale:}&mean&std&1D-Hist.&3D-Hist.&Grau-Hist.&Kantenz.\\
 \hline
 \textbf{Erfolg:} & 28\% & 32\% & 35\% & 38\% & 35\% & 41\% \\
 \hline
\end{tabular}
\caption{Genauigkeit der einzelnen Merkmale, gerundet}
\label{tab:gen}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|}
 \hline
 \textbf{Kombi.:}&mean+std&mean+std+Kantenz.&Kantenz.+3D-Hist.\\
 \hline
 \textbf{Erfolg:} & 40\% & 48\% & 49\% \\
 \hline
\end{tabular}
\caption{Genauigkeit von Merkmalskombinationen, gerundet}
\label{tab:gen2}
\end{table}

Abbildung~\ref{fig:meanstd} zeigt, dass die verschiedenen Klassen mithilfe von Mittelwert und Standardabweichung nur schwer auseinanderzuhalten sind und sich nicht leicht in verschiedene Klassen trennen lassen. Trotzdem erreichen wir hier nur mit diesen beiden Merkmalen eine Genauigkeit von 40\%.
\begin{figure}[h!]
\hspace*{-3cm}
\includegraphics[width=1.5\textwidth]{Scatterplot_mean_std.png}
\caption{Scatterplot für Mittelwert und Standardabweichung}
\label{fig:meanstd}
\end{figure}

Bei der Kantenzählung wurden beim Gaussschen Weichzeichner verschiedene Sigma Werte ausprobiert, wobei sich Sigma = 2 als am besten herausstellte (siehe Tabelle~\ref{tab:sigma}).
Dies ist so, da bei größeren Sigmas die Kanten zu sehr verschwimmen %<-- Bin mir da nicht 100% sicher, ob das stimmt, weil ich nicht weiß, ob ich Sigma richtig verstanden habe... || so hätte ich das auch verstanden - Max; ich auch - alo
\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
 \hline
 \textbf{Sigma:} & 0 & 1 & 2 & 3 & 4\\
 \hline
 \textbf{Erfolg:} & 40\% & 41\% & 41\% & 36\% & 35\% \\
 \hline
\end{tabular}
\caption{Genauigkeit der Kantenzählung bei verschiedenen Sigmas für den Weichzeichner}
\label{tab:sigma}
\end{table}

Die Kantenzählung war besonders darauf ausgelegt, stratiforme Wolken %(flächige Wolken)
zu identifizieren, da diese meist durchgängige Flächen sind und daher wenig Kanten haben.
Abbildung~\ref{fig:kbs} zeigt einen Boxplot und einen Swarmplot für die Kantenzählung.
Dabei ist wichtig zu berücksichtigen, dass für die Plots die Histogramme auf einen Wert reduziert wurden, das heißt der Plot spiegelt das Merkmal nur annäherungsweise wieder.
Während der Boxplot vermuten lässt, die stratiformen Wolken ließen sich mithilfe des Merkmals alleine klar von den anderen trennen, zeigt der Swarmplot, dass dies nur für etwa die Hälfte der Bilder mit stratiformen Wolken gilt. %Also haben wir das Merkmal so für unseren Entscheidungsbaum verwendet, dass wir alle Wolken die einen Gesamtkantenwert von unter 1000 haben als stratiform klassifizieren, dabei aber nicht ausschließen, dass unter den Verbliebenen noch stratiforme Wolken sind...könnte man hier schreiben wenn wir es machen. -- 

\begin{figure}[h!]
\centering
\subfigure[Boxplot Kantenzählung]
{\includegraphics[width=0.83\textwidth]{edge_count_boxplotLabeld.png}} \\
\subfigure[Swarmplot Kantenzählung]
{\includegraphics[width=0.83\textwidth]{edge_count_swarmplotLabeld.png}}
\caption{Unterschiede in den Kantenzählungswerten pro Klasse}
    \label{fig:kbs}
\end{figure}



\subsection{Gescheiterte Ansätze}
Wir hatten noch zusätzliche Merkmale implementiert, die nicht gut genug funktioniert haben, um sie zu verwenden.
Ein Ansatz war es das Region Growing anzuwenden, um so aus den Bildern die Zusammenhangskomponenten der Wolken auszuscheiden oder auch um die Frequenzen der Regionen aus diesen abzuleiten.
Besonders bei der Klassifizierung von Cumuluswolken
wäre die Anzahl der Zusammenhangskomponenten sehr aussagekräftig gewesen.
Wir konnten das Region Growing leider nicht mit unseren Daten verwenden, da auf vielen Bildern die Wolken ineinander übergehen und trotz Runterskalierung auf 50p x 50p, die Berechnung pro Bild noch immer bis zu einer Minute gedauert hat.


\subsection{Neuronale Netze}

Unser auf neuronalen Netzen basierendes System erreichte eine Klassifizierungsgenauigkeit von 65,6\% auf einem aus 20\% des Datensatzes bestehendem Testdatensatz, der kein Bild mit dem Trainingsdatensatz gemeinsam hat.

Aus der Konfusionsmatrix (Tabelle \ref{tab:confusionNeural}) ist zu erkennen, dass cumuliforme Wolken besonders gut erkannt werden, und cirriforme Wolken vergleichsweise schlecht.
Weiterhin wird ein recht hoher Anteil an Bildern fälschlicherweise als stratocumuliform klassifiziert (besonders cirriforme Wolken), ähnliches gilt für cumuliforme Wolken.

Wir vermuteten, dass dies an der recht ungleichen Verteilung der verschiedenen Labels im Datensatz liegt (siehe Tabelle \ref{table:oberklassen}).
Ein erneuter Testdurchlauf nach Angleichen der Klassengrößen im Trainingsdatensatz durch entfernen von Bildern aus überrepräsentierten Klassen bestätigte diese Vermutung zumindest teilweise, auch wenn die teils starken Schwankungen keine entgültigen Ergebnisse zulassen (siehe Tabelle \ref{table:confusionNeural2}).
Insgesamt lag die Genauigkeit bei 66,1\%, obwohl ja der Trainingsdatensatz verkleinert wurde.

% TODO: Wie schnell sind unsere Verfahren?
%Was passiert, wenn man einzelne Teile des Systems austauscht oder Merkmale entfernt (dies evtl.\ auch mit Diagrammen zeigen)?
%Wie wirken sich Änderungen der Parameter/Hyperpara\-meter auf die Ergebnisse aus?

\begin{table}
    \centering
    \begin{tabular}{| l | l | l | l | l |}
        \hline
        Label & cir & cum & str & s-c \\ \hline
        Cirriform (cir) & 40\% & 14\% & 2\% & 44\% \\ \hline
        Cumuliform (cum) & 2\% & 81\% & 2\% & 15\% \\ \hline
        Stratiform (str) & 5\% & 18\% & 56\% & 21\% \\ \hline
        Stratocumuliform (s-c) & 9\% & 24\% & 4\% & 64\% \\ \hline
    \end{tabular}
    \caption{Konfusionsmatrix des auf neuronalen Netzen basierenden Klassifikators. Zeilen sind die wirklichen Labels, Spalten die vom Klassifikator errechneten Labels. 80\% heißt, dass 80\% der Bilder mit dem gegebenen echten Label das gegebenen errechnete Label haben}
    \label{tab:confusionNeural}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{| l | l | l | l | l |}
        \hline
        Label & cir & cum & str & s-c \\ \hline
        Cirriform (cir) & 46\% & 24\% & 3\% & 27\% \\ \hline
        Cumuliform (cum) & 1\% & 83\% & 10\% & 6\% \\ \hline
        Stratiform (str) & 18\% & 18\% & 59\% & 5\% \\ \hline
        Stratocumuliform (s-c) & 10\% & 18\% & 16\% & 55\% \\ \hline
    \end{tabular}
    \caption{Konfusionsmatrix des auf neuronalen Netzen basierenden Klassifikators nach Angleichen der Klassengrößen im Training (vgl.\ Tabelle \ref{tab:confusionNeural})}
    \label{tab:confusionNeural2}
\end{table}


\subsection{Menschen klassifizieren Wolken}

Da uns aufgefallen ist, dass es nicht nur für den Computer sondern auch für uns schwierig ist, die Wolken zu klassifizieren, haben wir uns dazu entschieden, eine kleine Studie durchzuführen.
Bei dieser Studie sollten die sechs Probanden jeweils sechzehn Wolkenbilder in unsere vier Kategorien einteilen.
Sie haben die Übersicht aus Abbildung~\ref{fig:cloudtypes} bekommen und pro Klasse sechs Trainingsbilder als Referenz.
Dabei wurde eine durchschnittliche Genauigkeit von 51\% erreicht. Der Versuchsaufbau und die Ergebnisse sind noch einmal genauer im Anhang \ref{anhang:cloudgame} geschildert.

Die erreichte Genauigkeit von 51\% ist sehr nah an dem Ergebnis unseres klassischen Ansatzes dran, liegen überraschenderweise jedoch unter den Ergebnissen des Maschine-Learning-Ansatzes.
Zu beachten ist jedoch, dass die 36 Bilder, welche für die Testpersonen gewählt wurden, gut klassifizierbare Bilder waren.
Insbesondere umfassten sie nicht den gesamten Datensatz, wodurch die Ergebnisse nicht direkt vergleichbar sind.


\section{Fazit}
\label{sec:fazit}

Insgesamt können wir sagen, dass es eine Herausforderung war, die verschiedenen Bilder zu klassifizieren.
Nicht nur innerhalb einer Wolkenart unterscheiden sich die Bilder teils sehr stark voneinander, auch ist es vor allem die Ähnlichkeit vieler Bilder in jeweils verschiedenen Wolkenklassen, die die korrekte Klassifizierung der Wolken erschwert.
Und da es sogar für Menschen schwer ist, viele Bilder ihrer richtigen Kategorie zuzuordnen, hatten wir besonders bei der klassischen Methode nicht all zu genaue Ergebnisse erwartet.
Allerdings sind wir mit 49\% für die klassische Methodik schon sehr zufrieden, da dies noch weit besser ist, als ein zufälliges Zuordnen der Klassen.

Auch beim auf neuronalen Netzen aufbauenden Ansatz waren wir von der erreichten Leitung positiv überrascht, besonders in Hinblick auf den recht kleinen Datensatz.
Sofern sich die in der Entwicklung festgestellte Beziehung zwischen Datensatzgröße und erreichter Genauigkeit fortsetzt, wäre eine Leistung jenseits der 75\% nicht überraschend.
Weiterhin wäre es dann interessant, das zur Merkmalsextraktion genutzte CNN vollständig selbst zu trainieren; dies hatte die aktuelle Datensatzgröße nicht zugelassen.

Ebenso können wir uns vorstellen, dass ein viel besseres Ergebnis zu erreichen ist, wenn die Bilder alle nach einer bestimmten Norm aufgenommen wären, zum Beispiel mit dem selben Winkel zum Himmel, bei gleicher Helligkeit oder aber vom selben Standpunkt aus.
Jedoch sind dann auch deutliche Verluste bzgl.\ der Generalisierbarkeit auf andere Situationen zu erwarten.


\bibliographystyle{babplain}
\bibliography{bibliography}

\pagebreak
\appendix
\section{Cloud Game---Wie gut sind Menschen darin Wolken zu Klassifizieren}

Wir haben eine Studie durchgeführt, bei der wir sechs Personen, drei männlich/drei weiblich, im Alter von 19 bis 52 Jahren, gebeten haben, Wolken zu klassifizieren.
Die Probanden haben eine kurze Einführung in die Klassifikation von Wolken bekommen und wir haben Ihnen kurz erklärt, was wir in unserem Projekt tun (Klassifizierung von Wolken mittels Computer Vision).
Sonst hatten sie keine Vorkenntnisse.
Insgesamt erzielten sie eine Trefferquote von 51\%.


\subsection{Versuchsaufbau}
\label{anhang:cloudgame}

Den Probanden wurde Abbildung \ref{fig:cloudtypes} zur Verfügung gestellt.
Diese durften sie auch während der Klassifizierung nutzen.
Die verwendeten Testbilder finden sich auf den folgenden Seiten.
Es sind je zwölf Bilder für unsere vier Kategorien, also insgesamt 48 Bilder.

Die Probanden erhielten nun pro Kategorie sechs Trainingsbilder, die zufällig aus den zwölf gewählt wurden.
Die restlichen 24 Bilder wurden gemischt und dem Probanden wurden davon sechzehn gegeben, die er dann den Kategorien zuordnen sollte.
Die Testperson konnte so nicht einfach jeder Klasse die sechs fehlenden Bilder zuordnen, da acht Bilder fehlten.
Es hätte also auch passieren können, dass eine Kategorie nicht vertreten ist.
Dabei hatten die Probanden die 24 Trainingsbilder und die Übersicht als Hilfsmittel.
Der Versuch hat pro Person etwa eine Viertelstunde gedauert.

Der „Kluger-Hans-Effekt“ konnte nicht auftreten, da die testende Person selber nicht die richtigen Antworten wusste.
Die Ergebnisse des Versuches sind in der Konfusionsmatrix in Tabelle \ref{tab:humanConfusion} zu sehen.
Sechs Testpersonen sind natürlich zu wenige, um statistisch belastbare Ergebnisse zu erzielen, aber sie lassen zumindest eine Richtung erkennen.

\begin{table}[h!]
    \centering
    \begin{tabular}{| l | l | l | l | l |}
        \hline
        Label & cir & cum & str & s-c \\ \hline
        Cirriform (cir) & 56\% & 12\% & 20\% & 12\% \\ \hline
        Cumuliform (cum) & 0\% & 76\% & 4\% & 20\% \\ \hline
        Stratiform (str) & 18\% & 9\% & 45\% & 27\% \\ \hline
        Stratocumuliform (s-c) & 25\% & 13\% & 38\% & 25\% \\ \hline
    \end{tabular}
    \caption{Konfusionsmatrix bei der Klassifikation durch Menschen (N=6) (vgl.\ Tabelle \ref{tab:confusionNeural})}
    \label{tab:humanConfusion}
\end{table}

\includepdf[pages=3-]{CloudGame.pdf}


\end{document}


